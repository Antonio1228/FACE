{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facetorch import FaceAnalyzer\n",
    "from omegaconf import OmegaConf\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from typing import Dict\n",
    "import operator\n",
    "import torchvision\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config = \"gpu.config.yml\" \n",
    "cfg = OmegaConf.load(path_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_input = 'test.jpg'\n",
    "path_img_output = './outputs/result_image.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2024-03-26 10:58:25,023\", \"levelname\": \"INFO\", \"message\": \"Initializing FaceAnalyzer\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:25,023\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseReader\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2024-03-26 10:58:25,562\", \"levelname\": \"INFO\", \"message\": \"Initializing FaceDetector\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:29,016\", \"levelname\": \"INFO\", \"message\": \"Initializing FaceUnifier\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:29,061\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor objects\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:29,061\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor embed\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:29,745\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor verify\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:30,500\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor fer\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:31,253\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor au\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:31,963\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor va\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:32,640\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor deepfake\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:33,312\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor align\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:33,988\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer objects\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:33,999\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer align\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:35,528\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer draw_boxes\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:35,540\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer draw_landmarks\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:35,540\", \"levelname\": \"INFO\", \"message\": \"Running FaceAnalyzer\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:35,549\", \"levelname\": \"INFO\", \"message\": \"Reading image\", \"input\": \"test.jpg\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:35,897\", \"levelname\": \"INFO\", \"message\": \"Detecting faces\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:42,712\", \"levelname\": \"INFO\", \"message\": \"Number of faces: 4\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:42,712\", \"levelname\": \"INFO\", \"message\": \"Unifying faces\"}\n",
      "c:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194: UserWarning: operator () profile_node %403 : int = prim::profile_ivalue(%out_dtype.1)\n",
      " does not have profile information (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\jit\\codegen\\cuda\\graph_fuser.cpp:109.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "{\"asctime\": \"2024-03-26 10:58:43,021\", \"levelname\": \"INFO\", \"message\": \"Predicting facial features\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:43,021\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: embed\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:44,724\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: verify\"}\n",
      "{\"asctime\": \"2024-03-26 10:58:46,109\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: fer\"}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m FaceAnalyzer(cfg\u001b[38;5;241m.\u001b[39manalyzer)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# warmup\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_img_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_img_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_img_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_img_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_img_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\core.py:170\u001b[0m, in \u001b[0;36mFaceAnalyzer.run\u001b[1;34m(self, image_source, path_image, batch_size, fix_img_size, return_img_data, include_tensors, path_output, tensor)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m predictor_name, predictor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning FacePredictor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_predict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtilizing facial features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m utilizer_name, utilizer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutilizers\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\core.py:125\u001b[0m, in \u001b[0;36mFaceAnalyzer.run.<locals>._predict_batch\u001b[1;34m(data, predictor, predictor_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m     face_indx_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(face_indx_start \u001b[38;5;241m+\u001b[39m batch_size, n_faces)\n\u001b[0;32m    122\u001b[0m     face_batch_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m    123\u001b[0m         [face\u001b[38;5;241m.\u001b[39mtensor \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mfaces[face_indx_start:face_indx_end]]\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_batch_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     data\u001b[38;5;241m.\u001b[39madd_preds(preds, predictor_name, face_indx_start)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\predictor\\core.py:53\u001b[0m, in \u001b[0;36mFacePredictor.run\u001b[1;34m(self, faces)\u001b[0m\n\u001b[0;32m     51\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mrun(faces)\n\u001b[0;32m     52\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(faces)\n\u001b[1;32m---> 53\u001b[0m preds_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds_list\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\predictor\\post.py:115\u001b[0m, in \u001b[0;36mPostArgMax.run\u001b[1;34m(self, preds)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@Timer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPostArgMax.run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{milliseconds:.2f}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Prediction]:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes the prediction tensor using argmax and returns a list of prediction data structures, one for each face.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        List[Prediction]: List of prediction data structures containing the predicted labels and confidence scores for each face in the batch.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    116\u001b[0m     pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pred_list(preds, indices)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_list\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "analyzer = FaceAnalyzer(cfg.analyzer)\n",
    "# warmup\n",
    "response = analyzer.run(\n",
    "        path_image=path_img_input,\n",
    "        batch_size=cfg.batch_size,\n",
    "        fix_img_size=cfg.fix_img_size,\n",
    "        return_img_data=False,\n",
    "        include_tensors=True,\n",
    "        path_output=path_img_output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2024-03-13 00:58:00,302\", \"levelname\": \"INFO\", \"message\": \"Running FaceAnalyzer\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:00,304\", \"levelname\": \"INFO\", \"message\": \"Reading image\", \"input\": \"test.jpg\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:00,345\", \"levelname\": \"INFO\", \"message\": \"Detecting faces\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:04,782\", \"levelname\": \"INFO\", \"message\": \"Number of faces: 4\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:04,783\", \"levelname\": \"INFO\", \"message\": \"Unifying faces\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:04,785\", \"levelname\": \"INFO\", \"message\": \"Predicting facial features\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:04,786\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: embed\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:09,011\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: verify\"}\n",
      "{\"asctime\": \"2024-03-13 00:58:13,280\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: fer\"}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_img_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_img_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_img_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_img_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_img_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_img_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\core.py:170\u001b[0m, in \u001b[0;36mFaceAnalyzer.run\u001b[1;34m(self, image_source, path_image, batch_size, fix_img_size, return_img_data, include_tensors, path_output, tensor)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m predictor_name, predictor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning FacePredictor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_predict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtilizing facial features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m utilizer_name, utilizer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutilizers\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\core.py:125\u001b[0m, in \u001b[0;36mFaceAnalyzer.run.<locals>._predict_batch\u001b[1;34m(data, predictor, predictor_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m     face_indx_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(face_indx_start \u001b[38;5;241m+\u001b[39m batch_size, n_faces)\n\u001b[0;32m    122\u001b[0m     face_batch_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m    123\u001b[0m         [face\u001b[38;5;241m.\u001b[39mtensor \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mfaces[face_indx_start:face_indx_end]]\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_batch_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     data\u001b[38;5;241m.\u001b[39madd_preds(preds, predictor_name, face_indx_start)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\predictor\\core.py:53\u001b[0m, in \u001b[0;36mFacePredictor.run\u001b[1;34m(self, faces)\u001b[0m\n\u001b[0;32m     51\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mrun(faces)\n\u001b[0;32m     52\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(faces)\n\u001b[1;32m---> 53\u001b[0m preds_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds_list\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\predictor\\post.py:115\u001b[0m, in \u001b[0;36mPostArgMax.run\u001b[1;34m(self, preds)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@Timer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPostArgMax.run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{milliseconds:.2f}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Prediction]:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes the prediction tensor using argmax and returns a list of prediction data structures, one for each face.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        List[Prediction]: List of prediction data structures containing the predicted labels and confidence scores for each face in the batch.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    116\u001b[0m     pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pred_list(preds, indices)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_list\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "response = analyzer.run(\n",
    "        path_image=path_img_input,\n",
    "        batch_size=cfg.batch_size,\n",
    "        fix_img_size=cfg.fix_img_size,\n",
    "        return_img_data=cfg.return_img_data,\n",
    "        include_tensors=cfg.include_tensors,\n",
    "        path_output=path_img_output,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 假設錯誤發生在這一行之前\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds 的類型: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds 的內容: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "from facetorch import FaceAnalyzer\n",
    "from omegaconf import OmegaConf\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from typing import Dict\n",
    "import operator\n",
    "import torchvision\n",
    "import torch\n",
    "path_config = \"gpu.config.yml\" \n",
    "cfg = OmegaConf.load(path_config)\n",
    "path_img_input = 'human_picture//000010.jpg'\n",
    "path_img_output = 'test_output.jpg'\n",
    "# initialize\n",
    "analyzer = FaceAnalyzer(cfg.analyzer)\n",
    "# warmup\n",
    "response = analyzer.run(\n",
    "        path_image=path_img_input,\n",
    "        batch_size=cfg.batch_size,\n",
    "        fix_img_size=cfg.fix_img_size,\n",
    "        return_img_data=False,\n",
    "        include_tensors=True,\n",
    "        path_output=path_img_output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2024-03-14 10:53:17,624\", \"levelname\": \"INFO\", \"message\": \"Initializing FaceAnalyzer\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:17,626\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseReader\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:17,684\", \"levelname\": \"INFO\", \"message\": \"Initializing FaceDetector\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的配置: {'analyzer': {'device': 'cuda', 'optimize_transforms': True, 'reader': {'_target_': 'facetorch.analyzer.reader.ImageReader', 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'facetorch.transforms.SquarePad'}, {'_target_': 'torchvision.transforms.Resize', 'size': [1080], 'antialias': True}]}}, 'detector': {'_target_': 'facetorch.analyzer.detector.FaceDetector', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '1eMuOdGkiNCOUTiEbKKoPCHGCuDgiKeNC', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'reverse_colors': True, 'preprocessor': {'_target_': 'facetorch.analyzer.detector.pre.DetectorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Normalize', 'mean': [104.0, 117.0, 123.0], 'std': [1.0, 1.0, 1.0]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': '${analyzer.detector.reverse_colors}'}, 'postprocessor': {'_target_': 'facetorch.analyzer.detector.post.PostRetFace', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'confidence_threshold': 0.02, 'top_k': 5000, 'nms_threshold': 0.4, 'keep_top_k': 750, 'score_threshold': 0.6, 'prior_box': {'_target_': 'facetorch.analyzer.detector.post.PriorBox', 'min_sizes': [[16, 32], [64, 128], [256, 512]], 'steps': [8, 16, 32], 'clip': False}, 'variance': [0.1, 0.2], 'reverse_colors': '${analyzer.detector.reverse_colors}', 'expand_box_ratio': 0.0}}, 'unifier': {'_target_': 'facetorch.analyzer.unifier.FaceUnifier', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Normalize', 'mean': [-123.0, -117.0, -104.0], 'std': [255.0, 255.0, 255.0]}, {'_target_': 'torchvision.transforms.Resize', 'size': [380, 380], 'antialias': True}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': '${analyzer.optimize_transforms}'}, 'predictor': {'embed': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '19h3kqar1wlELAmM5hDyj9tlrUh8yjrCl', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [244, 244], 'antialias': True}, {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.228, 0.224, 0.225]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.embed.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': False}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostEmbedder', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.embed.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'labels': ['abstract']}}, 'verify': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '1WI-mP_0mGW31OHfriPUsuFS_usYh_W8p', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [112, 112], 'antialias': True}, {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.verify.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': True}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostEmbedder', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.verify.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'labels': ['abstract']}}, 'fer': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '1xoB5VYOd0XLjb-rQqqHWCkQvma4NytEd', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [260, 260], 'antialias': True}, {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.fer.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': False}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostArgMax', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.fer.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'dim': 1, 'labels': ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']}}, 'au': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '1uoVX9suSA5JVWTms3hEtJKzwO-CUR_jV', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [224, 224], 'antialias': True}, {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.au.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': False}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostMultiLabel', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.au.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'dim': 1, 'threshold': 0.5, 'labels': ['inner_brow_raiser', 'outer_brow_raiser', 'brow_lowerer', 'upper_lid_raiser', 'cheek_raiser', 'lid_tightener', 'nose_wrinkler', 'upper_lip_raiser', 'nasolabial_deepener', 'lip_corner_puller', 'sharp_lip_puller', 'dimpler', 'lip_corner_depressor', 'lower_lip_depressor', 'chin_raiser', 'lip_pucker', 'tongue_show', 'lip_stretcher', 'lip_funneler', 'lip_tightener', 'lip_pressor', 'lips_part', 'jaw_drop', 'mouth_stretch', 'lip_bite', 'nostril_dilator', 'nostril_compressor', 'left_inner_brow_raiser', 'right_inner_brow_raiser', 'left_outer_brow_raiser', 'right_outer_brow_raiser', 'left_brow_lowerer', 'right_brow_lowerer', 'left_cheek_raiser', 'right_cheek_raiser', 'left_upper_lip_raiser', 'right_upper_lip_raiser', 'left_nasolabial_deepener', 'right_nasolabial_deepener', 'left_dimpler', 'right_dimpler']}}, 'va': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '1Xl4ilNCU_DgKNhITrXb3UyQUUdm3VTKS', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [224, 224], 'antialias': True}, {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.va.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': False}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostLabelConfidencePairs', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.va.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'labels': ['valence', 'arousal'], 'offsets': [0, 0]}}, 'deepfake': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '1GjDTwQpvrkCjXOdiBy1oMkzm7nt-bXFg', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [380, 380], 'antialias': True}, {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': False}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostSigmoidBinary', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'labels': ['Real', 'Fake'], 'threshold': 0.7}}, 'align': {'_target_': 'facetorch.analyzer.predictor.FacePredictor', 'downloader': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '16gNFQdEH2nWvW3zTbdIAniKIbPAp6qBA', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/model.pt'}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'preprocessor': {'_target_': 'facetorch.analyzer.predictor.pre.PredictorPreProcessor', 'transform': {'_target_': 'torchvision.transforms.Compose', 'transforms': [{'_target_': 'torchvision.transforms.Resize', 'size': [120, 120], 'antialias': True}]}, 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.align.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'reverse_colors': False}, 'postprocessor': {'_target_': 'facetorch.analyzer.predictor.post.PostEmbedder', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.predictor.align.device.type}'}, 'optimize_transform': '${analyzer.optimize_transforms}', 'labels': ['abstract']}}}, 'utilizer': {'align': {'_target_': 'facetorch.analyzer.utilizer.align.Lmk3DMeshPose', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': False, 'downloader_meta': {'_target_': 'facetorch.downloader.DownloaderGDrive', 'file_id': '11tdAcFuSXqCCf58g52WT1Rpa8KuQwe2o', 'path_local': 'C:/Users/09350/OneDrive/文件/GitHub/FACE/meta.pt'}, 'image_size': 120}, 'draw_boxes': {'_target_': 'facetorch.analyzer.utilizer.draw.BoxDrawer', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': False, 'color': 'green', 'line_width': 3}, 'draw_landmarks': {'_target_': 'facetorch.analyzer.utilizer.draw.LandmarkDrawerTorch', 'transform': 'None', 'device': {'_target_': 'torch.device', 'type': '${analyzer.device}'}, 'optimize_transform': False, 'width': 2, 'color': 'green'}}, 'logger': {'_target_': 'facetorch.logger.LoggerJsonFile', 'name': 'facetorch', 'level': 20, 'path_file': '/opt/facetorch/logs/facetorch/main.log', 'json_format': '%(asctime)s %(levelname)s %(message)s'}}, 'main': {'sleep': 3}, 'debug': True, 'batch_size': 8, 'fix_img_size': True, 'return_img_data': True, 'include_tensors': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2024-03-14 10:53:19,303\", \"levelname\": \"INFO\", \"message\": \"Initializing FaceUnifier\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:19,364\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor objects\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:19,369\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor embed\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:20,104\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor verify\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:20,923\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor fer\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:21,722\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor au\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:22,526\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor va\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:23,394\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor deepfake\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:24,205\", \"levelname\": \"INFO\", \"message\": \"Initializing FacePredictor align\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:25,067\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer objects\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:25,069\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer align\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:25,155\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer draw_boxes\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:25,160\", \"levelname\": \"INFO\", \"message\": \"Initializing BaseUtilizer draw_landmarks\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:25,179\", \"levelname\": \"INFO\", \"message\": \"Running FaceAnalyzer\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:25,181\", \"levelname\": \"INFO\", \"message\": \"Reading image\", \"input\": \"human_picture//000010.jpg\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化後的分析器配置: <facetorch.analyzer.core.FaceAnalyzer object at 0x000001E3039068B0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2024-03-14 10:53:25,393\", \"levelname\": \"INFO\", \"message\": \"Detecting faces\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:26,581\", \"levelname\": \"INFO\", \"message\": \"Number of faces: 1\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:26,583\", \"levelname\": \"INFO\", \"message\": \"Unifying faces\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:26,829\", \"levelname\": \"INFO\", \"message\": \"Predicting facial features\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:26,831\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: embed\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:27,182\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: verify\"}\n",
      "{\"asctime\": \"2024-03-14 10:53:27,591\", \"levelname\": \"INFO\", \"message\": \"Running FacePredictor: fer\"}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m初始化後的分析器配置: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# warmup\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_img_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_img_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_img_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_img_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_img_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 列印 response 來檢查輸出類型和內容\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m分析結果: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\core.py:170\u001b[0m, in \u001b[0;36mFaceAnalyzer.run\u001b[1;34m(self, image_source, path_image, batch_size, fix_img_size, return_img_data, include_tensors, path_output, tensor)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m predictor_name, predictor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning FacePredictor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_predict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtilizing facial features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m utilizer_name, utilizer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutilizers\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\core.py:125\u001b[0m, in \u001b[0;36mFaceAnalyzer.run.<locals>._predict_batch\u001b[1;34m(data, predictor, predictor_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m     face_indx_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(face_indx_start \u001b[38;5;241m+\u001b[39m batch_size, n_faces)\n\u001b[0;32m    122\u001b[0m     face_batch_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m    123\u001b[0m         [face\u001b[38;5;241m.\u001b[39mtensor \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mfaces[face_indx_start:face_indx_end]]\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_batch_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     data\u001b[38;5;241m.\u001b[39madd_preds(preds, predictor_name, face_indx_start)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\predictor\\core.py:53\u001b[0m, in \u001b[0;36mFacePredictor.run\u001b[1;34m(self, faces)\u001b[0m\n\u001b[0;32m     51\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mrun(faces)\n\u001b[0;32m     52\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(faces)\n\u001b[1;32m---> 53\u001b[0m preds_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds_list\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\09350\\.conda\\envs\\myenv\\lib\\site-packages\\facetorch\\analyzer\\predictor\\post.py:115\u001b[0m, in \u001b[0;36mPostArgMax.run\u001b[1;34m(self, preds)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@Timer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPostArgMax.run\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{milliseconds:.2f}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Prediction]:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes the prediction tensor using argmax and returns a list of prediction data structures, one for each face.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        List[Prediction]: List of prediction data structures containing the predicted labels and confidence scores for each face in the batch.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    116\u001b[0m     pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pred_list(preds, indices)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_list\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "from facetorch import FaceAnalyzer\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "path_config = \"gpu.config.yml\"\n",
    "cfg = OmegaConf.load(path_config)\n",
    "path_img_input = 'human_picture//000010.jpg'\n",
    "path_img_output = 'test_output.jpg'\n",
    "\n",
    "# 列印 cfg 以確認配置是否正確\n",
    "print(f\"使用的配置: {cfg}\")\n",
    "\n",
    "# 初始化\n",
    "analyzer = FaceAnalyzer(cfg.analyzer)\n",
    "\n",
    "# 列印初始化後的分析器配置，確認無誤\n",
    "print(f\"初始化後的分析器配置: {analyzer}\")\n",
    "\n",
    "# warmup\n",
    "response = analyzer.run(\n",
    "    path_image=path_img_input,\n",
    "    batch_size=cfg.batch_size,\n",
    "    fix_img_size=cfg.fix_img_size,\n",
    "    return_img_data=False,\n",
    "    include_tensors=True,\n",
    "    path_output=path_img_output,\n",
    ")\n",
    "\n",
    "# 列印 response 來檢查輸出類型和內容\n",
    "print(f\"分析結果: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
