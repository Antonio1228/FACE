{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb459b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 29.42it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 27.63it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.26it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.54it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.91it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.08it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.85it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.01it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.80it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 29.37it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.12it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.60it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.72it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.47it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.09it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.44it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 36.19it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.56it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.79it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.78it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.67it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.37it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.16it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.47it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.20it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 29.44it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.33it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.35it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.96it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.34it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.10it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.68it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.67it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.46it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.32it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.15it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.40it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.09it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.34it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.27it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.69it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.74it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.73it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.23it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.72it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.95it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.70it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.40it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.03it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.29it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.64it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.67it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.01it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.11it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.00it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.09it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.16it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.87it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.70it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.32it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.79it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.54it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.62it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.09it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.54it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.42it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.51it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.05it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.51it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.27it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 33.89it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.72it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.09it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.65it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.15it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.45it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.15it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.55it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.10it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 27.87it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.23it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.36it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.54it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.81it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.62it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 29.84it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.50it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 29.28it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.19it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.29it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.93it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.78it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 34.65it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.83it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 35.12it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.78it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.52it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.03it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.92it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 30.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，結果已儲存到 Human.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from deepface import DeepFace\n",
    "\n",
    "def classify_eye_size(eye_area, threshold=650):\n",
    "    # 根據面積分類眼睛大小\n",
    "    if eye_area > threshold:\n",
    "        return 'big'\n",
    "    else:\n",
    "        return 'small'\n",
    "\n",
    "def analyze_emotion(img_path):\n",
    "    # 使用 DeepFace 分析表情\n",
    "    try:\n",
    "        result = DeepFace.analyze(img_path=img_path, actions=['emotion'], enforce_detection=False)\n",
    "        if result: \n",
    "            emotions = result[0]['emotion']\n",
    "            highest_emotion = max(emotions, key=emotions.get)\n",
    "            return highest_emotion\n",
    "        else:\n",
    "            return \"No Face Detected\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def mouth_open(mouth):\n",
    "    # 判斷嘴巴是否開合\n",
    "    vertical_dist = np.linalg.norm(mouth[7] - mouth[2])\n",
    "    horizontal_dist = np.linalg.norm(mouth[0] - mouth[4])\n",
    "    ratio = float(vertical_dist) / float(horizontal_dist)\n",
    "    return ratio > 0.8\n",
    "\n",
    "# 初始化 dlib 的臉部檢測器和特徵點檢測器\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # 更換為您的特徵點模型路徑\n",
    "\n",
    "# 初始化 Haar Cascade 分類器\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# 指定資料夾路徑\n",
    "folder_path = 'human_picture'  # 更改為您的圖片資料夾路徑\n",
    "\n",
    "# CSV 檔案路徑\n",
    "csv_file = 'Human.csv'\n",
    "\n",
    "# 遍歷資料夾中的所有圖片\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Image', 'Eye Size', 'Highest Emotion', 'Mouth State'])  # 寫入標題\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            faces = detector(gray)\n",
    "            eye_sizes = []\n",
    "            mouth_state = \"No Face Detected\"\n",
    "\n",
    "            for face in faces:\n",
    "                landmarks = predictor(gray, face)\n",
    "                # 眼睛大小分類\n",
    "                roi_gray = gray[face.top():face.bottom(), face.left():face.right()]\n",
    "                eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "                for (ex,ey,ew,eh) in eyes:\n",
    "                    eye_area = ew * eh\n",
    "                    eye_size = classify_eye_size(eye_area)\n",
    "                    eye_sizes.append(eye_size)\n",
    "\n",
    "                # 嘴巴開合檢測\n",
    "                mouth_points = np.array([[p.x, p.y] for p in landmarks.parts()[48:68]])\n",
    "                mouth_state = \"open\" if mouth_open(mouth_points) else \"close\"\n",
    "\n",
    "            emotion_result = analyze_emotion(img_path)\n",
    "\n",
    "            # 將結果寫入 CSV 檔案\n",
    "            if eye_sizes:\n",
    "                writer.writerow([filename, eye_sizes[0], emotion_result, mouth_state])\n",
    "            else:\n",
    "                writer.writerow([filename, \"No Eyes Detected\", emotion_result, mouth_state])\n",
    "\n",
    "print(\"處理完成，結果已儲存到\", csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37949c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
